{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGRMGMT 590.10 Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due at 6:15pm ET on Thursday, March 21*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ There are 9 exercises in total. Exercise 1 through 3 require coding. Your grades are based on the accuracy of the results, not the code execution speed.\n",
    "  + Exercise 1: 1 point.\n",
    "  + Exercise 2 and 3: each is worth 3 points. \n",
    "  + Exercise 4 to 9: each is worth 0.5 points. \n",
    "+ Exercise 1 through 3 require generating model parameters using code you built **from scratch** without relying on existing modules with functions or capabilities to \"plug-in and populate\" parameters for those models.\n",
    "+ As the coding exercises require matrix manipulation, familiarize yourself with the following functions and methods:\n",
    "  + numpy.linalg.inv\n",
    "  + numpy.ndarray.dot\n",
    "  + numpy.dot\n",
    "  + numpy.ndarray.T   \n",
    "+ Exercises are independent with each other. Feel free to work on them in the order you prefer.\n",
    "+ Submit the .ipynb file to Sakai before 6:15pm ET on Thursday, March 21.\n",
    "+ Assignments handed in late will lose 1 point every 24-hour window after 6:15pm ET on Thursday, March 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Information:**\n",
    "\n",
    "We'll be working on three datasets attached to Assignment 4 on Sakai. Each coding exercise requires a different dataset:\n",
    "+ Exercise 1: 'Assignment4_SampleData_Ex1.csv'\n",
    "+ Exercise 2: 'Assignment4_SampleData_Ex2.csv'\n",
    "+ Exercise 3: 'Assignment4_SampleData_Ex3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Read the data from 'Assignment4_SampleData_Ex1.csv' to a pandas DataFrame df1\n",
    "+ Create the code from scratch to calculate the intercept and slope for a simple linear regression model with the input variable X and output variable Y in df1 using the Normal Equation approach\n",
    "+ Compare the intercept and slope you calculated with the results returned by sklearn's LinearRegression\n",
    "+ No need to split the data into training and testing for this exercise\n",
    "+ *Hint: refer to slide 16 in the week 7 deck. If calculated correctly, your parameters should be exactly the same as those returned by sklearn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept and slope according to the Normal Equation\n",
      "intercept: 5.215096157546816 slope: 8.934318110410974\n",
      "\n",
      "The intercept and slope according to SKLearn\n",
      "intercept: [5.21509616] slope: [[8.93431811]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "df1=pd.read_csv('/Users/paras/Desktop/Assignment4_SampleData_Ex1.csv')\n",
    "b=np.array(df1.X)\n",
    "b=b.reshape(b.shape[0],1)\n",
    "y=np.array(df1.Y)\n",
    "x = np.concatenate((np.ones((b.shape[0],1)), b), axis = 1)\n",
    "theta=np.dot(np.dot(np.linalg.inv(np.dot(x.T,x)),x.T),y)\n",
    "x_new=theta[1]*x + theta[0]\n",
    "_=plt.plot(x[:,1],x_new[:,1])\n",
    "_=sns.scatterplot(x[:,1],y,color=\"R\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The intercept and slope according to the Normal Equation\")\n",
    "print('intercept: '+ str(theta[0])+ ' slope: '+ str(theta[1]))\n",
    "\n",
    "X=pd.DataFrame(df1.X)\n",
    "y=pd.DataFrame(df1.Y)\n",
    "linearRegressor = LinearRegression()\n",
    "\"\"\"for i in range(10000):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.2,random_state=i)\"\"\"\n",
    "\n",
    "linearRegressor.fit(X, y)\n",
    "print(\"\\nThe intercept and slope according to SKLearn\")\n",
    "print('intercept: '+ str(linearRegressor.intercept_)+ ' slope: '+ str(linearRegressor.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Read the data from 'Assignment4_SampleData_Ex2.csv' to a pandas DataFrame df2\n",
    "+ Create a function BGD from scratch to estimate the intercept and slopes for a multiple linear regression model with X1 and X2 as the input variables and Y as the output variable using the Batch Gradient Descent approach\n",
    "+ Compare your estimates with the results returned by sklearn's LinearRegression\n",
    "+ No need to split the data into training and testing for this exercise\n",
    "+ *Hint: refer to slide 26-30 in the week 7 deck. If implemented correctly, your parameters should be similar to those returned by sklearn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BDG implementation [4.88825037 5.8408434  9.105232  ]\n",
      "Linear Regression from scikit learn: 4.910610036450066 [5.82913734 9.10275877]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def BGD(X, Y, eta, n_rounds, tol, random_state):\n",
    "    \"\"\"\n",
    "    Returns the intercept and predictor coefficients for a multiple linear regression model using Batch Gradient Descent \n",
    "    with MSE as the cost function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: a numpy ndarray with the independent variables\n",
    "    Y: a numpy ndarray with the dependent variable\n",
    "    eta: the learning rate, e.g. 0.1, 0.01, 0.001 etc.\n",
    "    n_rounds: number of passes over the data, e.g. 100, 1000, 10000 etc.\n",
    "    tol: the stopping criteria, i.e. previous MSE - current MSE < tol\n",
    "    random_state: a random state to make the randomness deterministic\n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    BGD(X, Y)\n",
    "    BGD(X, Y, eta = 0.01)\n",
    "    \n",
    "    Notes\n",
    "    ----------\n",
    "    The parameters are returned in a numpy ndarray.\n",
    "    \n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = X.shape[1]\n",
    "    one_column = np.ones((X.shape[0],1))\n",
    "    X = np.concatenate((one_column, X), axis = 1)\n",
    "    theta=np.random.rand(n+1)\n",
    "    h = hypothesis(theta, X, n)\n",
    "    theta,cost = gd(theta,eta,n_rounds,h,X,y,n,tol)\n",
    "    return theta,cost\n",
    "\n",
    "def gd(theta, eta, n_rounds, h, X, y, n,tol):\n",
    "    cost = np.ones(n_rounds)\n",
    "    for i in range(0,n_rounds):\n",
    "        theta[0] = theta[0] - (eta/X.shape[0]) * sum(h - y)\n",
    "        for j in range(1,n+1):\n",
    "            theta[j] = theta[j] - (eta/X.shape[0]) * sum((h-y) * X.transpose()[j])\n",
    "        h = hypothesis(theta, X, n)\n",
    "        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))\n",
    "        if abs(cost[i-1]-cost[i])<tol:\n",
    "            break\n",
    "    return theta,cost\n",
    "\n",
    "def hypothesis(theta, X, n):\n",
    "    h = np.ones((X.shape[0],1))\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    for i in range(0,X.shape[0]):\n",
    "        h[i] = float(np.dot(theta, X[i]))\n",
    "    h = h.reshape(X.shape[0])\n",
    "    return h\n",
    "\n",
    "\n",
    "df2=pd.read_csv('/Users/paras/Desktop/Assignment4_SampleData_Ex2.csv')\n",
    "x=df2.iloc[:,1:]\n",
    "y=df2.iloc[:,0]\n",
    "a,b=BGD(x,y,0.1,1000,0.000001,42)\n",
    "print('Custom BDG implementation '+ str(a))\n",
    "linearRegressor.fit(x,y)\n",
    "print('Linear Regression from scikit learn: ' +str(linearRegressor.intercept_) + ' ' + str(linearRegressor.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Read the data from 'Assignment4_SampleData_Ex3.csv' to a pandas DataFrame df3\n",
    "+ Create a function BGD_LogisticRegression from scratch to estimate the intercept and slopes for a binary logistic regression model with X1, X2 and X3 as the input variables and Y as the output variable using the Batch Gradient Descent approach\n",
    "+ Compare the F1 score of the logistic regression models using the parameters returned by BGD_LogisticRegression vs. sklearn's LogisticRegression\n",
    "+ No need to split the data into training and testing for this exercise\n",
    "+ *Hint: refer to slide 58-62 in the week 7 deck. If implemented correctly, your F1 score should be close to that returned by sklearn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept and the slopes through custom bgd are [[3.56817585 1.25764884 4.47655341 5.95248491]]\n",
      "The intercept and the slopes throug scikit learn are [3.71164161] [[1.33153031 4.66805387 6.21836126]]\n",
      "\n",
      "The f1 score calculated through custom bgd implementation is 0.9732484076433122\n",
      "The f1 score calculated through scikit learn logistic regression is :0.9725940089228807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def BGD_LogisticRegression(X, y, eta, n_rounds, tol, random_state):\n",
    "    \"\"\"\n",
    "    Returns the intercept and predictor coefficients for a binary logistic regression model using Batch Gradient Descent \n",
    "    with the log loss as the cost function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: a numpy ndarray with the independent variables\n",
    "    Y: a numpy ndarray with the dependent binary categorical variable\n",
    "    eta: the learning rate, e.g. 0.1, 0.01, 0.001 etc.\n",
    "    n_rounds: number of passes over the data, e.g. 100, 1000, 10000 etc. \n",
    "    tol: the stopping criteria, i.e. previous log loss - current log loss < tol\n",
    "    random_state: a random state to make the randomness deterministic\n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    BGD_LogisticRegression(X, Y)\n",
    "    BGD_LogisticRegression(X, Y, eta = 0.001)\n",
    "    \n",
    "    Notes\n",
    "    ----------\n",
    "    The parameters are returned in a numpy ndarray.\n",
    "    \n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = X.shape[1]\n",
    "    one_column = np.ones((X.shape[0],1))\n",
    "    X = np.concatenate((one_column, X), axis = 1)\n",
    "    theta = np.random.rand(n+1)\n",
    "    h = hypothesis1(theta, X, n)\n",
    "    theta,cost = BGD1(theta,eta,n_rounds,h,X,y,n,tol)\n",
    "    return theta, cost\n",
    "        \n",
    "\n",
    "    \n",
    "def BGD1(theta, eta, n_rounds, h, X, y, n,tol):\n",
    "    cost = np.ones(n_rounds)\n",
    "    for i in range(0,n_rounds):\n",
    "        theta[0] = theta[0] - (eta/X.shape[0]) * sum(h - y)\n",
    "        for j in range(1,n+1):\n",
    "            theta[j]=theta[j]-(eta/X.shape[0])*sum((h-y)*X.transpose()[j])\n",
    "        h = hypothesis1(theta, X, n)\n",
    "        cost[i]=(-1/X.shape[0])*sum(y*np.log(h)+(1-y)*np.log(1 - h))\n",
    "        if abs(cost[i-1]-cost[i])<tol:\n",
    "            break\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    return theta, cost\n",
    "\n",
    "\n",
    "def hypothesis1(theta, X, n):\n",
    "    h = np.ones((X.shape[0],1))\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    for i in range(0,X.shape[0]):\n",
    "        h[i] = float(1 / (1 + np.exp(-float(np.dot(theta, X[i])))))\n",
    "    h = h.reshape(X.shape[0])\n",
    "    return h\n",
    "\n",
    "df3=pd.read_csv('/Users/paras/Desktop/Assignment4_SampleData_Ex3.csv')\n",
    "X=df3.iloc[:,1:]\n",
    "Y=df3.iloc[:,0]\n",
    "\n",
    "x=df3.iloc[:,1:]\n",
    "a,b=BGD_LogisticRegression(X, Y, 0.1,3000,0.00000001,42)\n",
    "X = np.concatenate((np.ones((X.shape[0],1)), X), axis = 1)\n",
    "h = hypothesis1(a, X, X.shape[1] - 1)\n",
    "for i in range(0, h.shape[0]):\n",
    "    if h[i] > 0.5:\n",
    "        h[i] = 1\n",
    "    else:\n",
    "        h[i] = 0\n",
    "\n",
    "print('The intercept and the slopes through custom bgd are ' + str(a))\n",
    "f1_score111=f1_score(Y,h)\n",
    "lg=LogisticRegression(random_state=42)\n",
    "lg.fit(x,Y)\n",
    "predicted = lg.predict(x)\n",
    "f=f1_score(Y,predicted)\n",
    "\n",
    "print('The intercept and the slopes throug scikit learn are ' +str(lg.intercept_) + ' ' +str(lg.coef_) + '\\n')\n",
    "print('The f1 score calculated through custom bgd implementation is ' + str(f1_score111))\n",
    "print('The f1 score calculated through scikit learn logistic regression is :'+str(f))\n",
    "\n",
    "\"\"\"To make it more precise increase the n_rounds but this would take up the computation time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all Gradient Descent algorithms result in the same model if you run them long enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"If it is  Linear Regression or Logistic Regression, and assuming\n",
    "the learning rate is not too high, then all Gradient Descent algorithms will approach the global optimum \n",
    "and end up producing fairly similar models. However, unless you gradually reduce the learning rate, \n",
    "Stochastic GD and Mini-batch GD will never truly converge; instead, they will keep jumping back and forth \n",
    "around the global optimum. This means that even if you let them run for a very long time, these \n",
    "Gradient Descent algorithms will produce slightly different models.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Explain the main difference between the Ridge, Lasso, and Elastic Net regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Ridge and Lasso regression use two different penalty functions. Ridge uses l2 where as lasso goes with l1. \n",
    "In ridge regression, the penalty is the sum of the squares of the coefficients and for the Lasso, \n",
    "itâ€™s the sum of the absolute values of the coefficients. ElasticNet is hybrid of lasso and ridge regression . \n",
    "It is trained with L1 and L2 prior as regularizer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does increasing the hyperparameter of the Lasso regression impact the model's bias and variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With the increase in hyperparameter, there is an increase in bias with a decrease in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What is a Support Vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The vectors that define the hyperplane are the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it important to scale the input features when using SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"SVM tries to maximize the distance between the separating plane and the support vectors. \n",
    "If one feature has very large values, it will dominate the other features when calculating the distance. \n",
    "If you rescale all features (e.g. to [0, 1]), they all have the same influence on the distance metric\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What is the main difference between the hard and soft margin classifers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Hard-margin\n",
    "This assumes that data is very well behaved, and you can find a perfect classifier - which will have \n",
    "0 error on train data.\n",
    "\n",
    "Soft-margin\n",
    "Data is usually not well behaved, so SVM hard margin may not have any solution at all. \n",
    "So we allow for a little bit of error on some points. So the training error will not be 0, \n",
    "but average error over all points is minimized. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
